{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTWD3fgDvLZf4n/yNFmrZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micheldc55/Deep-Learning/blob/main/deep_learning_fundamentals/self_attention_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the attention mechanism\n",
        "\n",
        "The idea of this small project is to show how the attention mechanism works at a small scale. Attention is this cool feature that was first introduced in 2015 for Recurrent Neural Networks (RNNs), and that is the base building block of the Transformer arquitecture.\n",
        "\n",
        "First, we will take a look into BERT the original encoder model published by Google in 2018. And we will see how the attention block of the transformer is built. We will notice that it's just a simple attention block, similar to the one discussed in the famous [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) paper.\n",
        "\n",
        "We need to install the **transformers** library from HuggingFace in order to do so, so simply run the command below:"
      ],
      "metadata": {
        "id": "vzSwyzcts6FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUOTT2rPSqBh",
        "outputId": "f3af6192-b591-4a26-ba7d-e19c42387fb1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "lQXpbBheVBXP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Attention in BERT's first layer\n",
        "\n",
        "Below we will just import the BERT uncased model from HuggingFace. Let's explore how the model's layers are built. The BERT base uncased model is a pre-trained model from Google that can be used for feature extraction. It converts de prompt we pass to the model to a"
      ],
      "metadata": {
        "id": "4x54ORWITt-G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "QcC4xI4pSX4c"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7x1tS95SfCq",
        "outputId": "c1adb849-c3f1-4215-89d1-52fa7efda14c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen below, BERT has 12 attention layers in the enconder phase in total. Each of this attention layers can be though of as a creating a deeper and more context-aware representation of the input, based on the raw values of the input as individual tokens."
      ],
      "metadata": {
        "id": "E_ikqHT_T6hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.encoder.layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-yOGRcoSoHD",
        "outputId": "2f8613bb-8382-4bec-bf4e-ad8cb6b3c5d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention mechanism breaks the initial input into 3 matrices Q, K and V. These 3 matrices are used to generate a representation of how much \"attention\" should be payed to each of the input tokens, based on the embeddings for each token on its own."
      ],
      "metadata": {
        "id": "tuqv4-GsUnym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.layer[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IShrwUM_S6Ut",
        "outputId": "ae523e79-c3f6-4021-8dfb-aa8c42499d93"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertLayer(\n",
              "  (attention): BertAttention(\n",
              "    (self): BertSelfAttention(\n",
              "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (output): BertSelfOutput(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (intermediate): BertIntermediate(\n",
              "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    (intermediate_act_fn): GELUActivation()\n",
              "  )\n",
              "  (output): BertOutput(\n",
              "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of the Q, K and V matrices given an input\n",
        "\n",
        "Let's create an input phrase, for example: \"I am short\". Now let's create the corresponding spaces Q, K and V for this tokens, and see how that would play out. The input has 3 words (and we will treat words like tokens for now), so we will have our input \"X\" be 3 x 9 where 9 is just our vocabulary, our \"number of possible words\", if you will.\n",
        "\n",
        "**Note:** In reality, words will actually be tokens, and the vocabulary will be way larger than 9, but 9 will serve as a good way to show how we would proceed if this was the real transformer."
      ],
      "metadata": {
        "id": "6VZW7oXkVZCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"I am short\""
      ],
      "metadata": {
        "id": "_z2c_3UBS9md"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This would get broken down to:"
      ],
      "metadata": {
        "id": "eDm22YUHZ-_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|       | d1    | d2     | d3 | d4 | d5 | d6  | d7 | d8 | d9    |\n",
        "|-------|-------|--------|----|----|----|-----|----|----|--------|\n",
        "| I     | 0.8137| 0.7803 | ...| ...| ...| ... | ...| ...| 0.117  |\n",
        "| am    | 0.1000| 0.2150 | ...| ...| ...| ... | ...| ...| 0.701  |\n",
        "| short | 0.1770|-0.7047 | ...| ...| ...| ... | ...| ...| -0.321 |"
      ],
      "metadata": {
        "id": "VgRZmcSva-hB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where ${d_i}$ are the different dimensions of each input token in positinal encoding embedding space. Check the [attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) paper for a reference to the actual architecture image."
      ],
      "metadata": {
        "id": "TL8_KoRnbKuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use PyTorch to build such matrix and go over the process that would happen inside a Transformer. We will call our inputs \"X\", and our Query, Key and Value matrices Q, K and V respectively, just like the paper does.\n",
        "\n",
        "The attention mechanism is based on calculating the attention value that should be applied to each value in the \"Value\" space. If we think of V as if it holds the information of the meaning of the words independently of their context, we want to train a normalized attention matrix ${\\frac{QK^{T}}{\\sqrt{d_{k}}}}$ that measures what tokens should be payed the most attention to. The output is converted to probabilities using a softmax transformation.\n",
        "\n",
        "The attention matrix will then multiply the Value matrix to convert the context agnostic \"V\" matrix to a context-aware \"V\" matrix. So, the attention applied to the V matrix can be written as:\n",
        "\n",
        "$${softmax(\\frac{QK^{T}}{\\sqrt{d_{k}}})V}$$"
      ],
      "metadata": {
        "id": "BZCvn7PPbxzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need three extra matrices that will not be discussed for now. This are the weights matrices, and they multiply or input \"X\" and transform it to Q, K and V respectively. They are generally written as ${W^{Q}}$, ${W^{K}}$ and ${W^{V}}$"
      ],
      "metadata": {
        "id": "YgNL6JkDeXWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So imagine that the dimensions of each token in the positional embedding space is:"
      ],
      "metadata": {
        "id": "d8sHZi1MeqXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.Tensor([[0.8137, 0.7803, 0.0972, 0.1619, 0.6793, 0.0919, 0.4634, 0.5565, 0.117],\n",
        "        [0.1000, 0.2150, 0.5824, 0.5941, 0.7734, 0.1016, 0.9998, 0.1266, 0.701],\n",
        "        [0.1770, -0.7047, 0.7806, 0.4591, 0.3710, 0.1028, 0.0787, 0.0687, -0.321]])\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbPVtbDRZ-Tk",
        "outputId": "7114a18f-c13f-41d8-df5a-0ba375d911e6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.8137,  0.7803,  0.0972,  0.1619,  0.6793,  0.0919,  0.4634,  0.5565,\n",
              "          0.1170],\n",
              "        [ 0.1000,  0.2150,  0.5824,  0.5941,  0.7734,  0.1016,  0.9998,  0.1266,\n",
              "          0.7010],\n",
              "        [ 0.1770, -0.7047,  0.7806,  0.4591,  0.3710,  0.1028,  0.0787,  0.0687,\n",
              "         -0.3210]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, \"I\" in the initial embedding space is represented by the 9 dimensional vector: [0.8137,  0.7803,  0.0972,  0.1619,  0.6793,  0.0919,  0.4634,  0.5565, 0.1170]\n",
        "\n",
        "\"am\" in the same space is represented by the 9 dimensional vector: [0.1000,  0.2150,  0.5824,  0.5941,  0.7734,  0.1016,  0.9998,  0.1266, 0.7010]\n",
        "\n",
        "And \"short\" is represented by the vector: [0.1770, -0.7047,  0.7806,  0.4591,  0.3710,  0.1028,  0.0787,  0.0687, -0.3210]"
      ],
      "metadata": {
        "id": "T4FLGeeagKKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we imagine we want to create a representation of the text in a 4-dimensional space, then our Weight vectors (${W^{i}}$) will be of the shape 9x4.\n",
        "\n",
        "So the shape of Q, K and V, will be 3 x 4, since they will be the product of multiplying X times their corresponding weight matrix, and we know the dimensions of both. Let's create some artificial Q, K and V matrices:"
      ],
      "metadata": {
        "id": "lALeHxU9hRqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q = torch.Tensor([[0.023, 0.7144, 1.03, 0.576], [0.903, 0.1474, -0.338, -0.765], [0.576, -0.7144, 0.03, -0.076]])\n",
        "\n",
        "K = torch.Tensor([[0.5224, -0.1328, 0.9169, 0.5317], [0.3535, 0.7698, -0.2965, 0.2669], [0.3864, 0.5015, 0.5557, -0.1671]])\n",
        "\n",
        "V = torch.Tensor([[0.7467, 0.0360, -0.5847, 0.0370], [1.509, 0.8009, 0.7957, 0.0151], [0.4343, 0.9003, 0.3387, 0.2031]])"
      ],
      "metadata": {
        "id": "RXgtl5zofLJf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically, Q, K and V are the representations of the input embedding vectors in the Q space, K space and V spaces respectively. The V space attempts to retain information about the individual tokens while the other two combine to generate a representation of the tokens in their context and will be the building blocks for the attention mechanism applied to V.\n",
        "\n",
        "Let's now build ${QK^{T}}$"
      ],
      "metadata": {
        "id": "bozragBTjLvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K_t = torch.transpose(K, 0, 1)\n",
        "K_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfONJsXUicnK",
        "outputId": "a876a50d-ebbb-4bfb-b7da-ac72613d77ad"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5224,  0.3535,  0.3864],\n",
              "        [-0.1328,  0.7698,  0.5015],\n",
              "        [ 0.9169, -0.2965,  0.5557],\n",
              "        [ 0.5317,  0.2669, -0.1671]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_kt = torch.mm(Q, K_t)\n",
        "q_kt  # This is a 3x3 matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0EyUXQziiJK",
        "outputId": "20dea4cc-0eac-4486-d6e2-6560c0eb0f62"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.1678,  0.4064,  0.8433],\n",
              "        [-0.2645,  0.3287,  0.3628],\n",
              "        [ 0.3829, -0.3755, -0.1063]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch allows us to calculate the attention matrix using a few simple functions:"
      ],
      "metadata": {
        "id": "WHkNhN52kJbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_space = q_kt.shape[1]\n",
        "\n",
        "att_before_softmax = torch.div(q_kt, torch.sqrt(torch.tensor(embedding_space)))\n",
        "\n",
        "att_before_softmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQdKGkEKjniE",
        "outputId": "e405b8a5-0664-449f-cd18-7a9e57b70ea9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6742,  0.2346,  0.4869],\n",
              "        [-0.1527,  0.1898,  0.2095],\n",
              "        [ 0.2211, -0.2168, -0.0614]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that for the word \"I\" for example (first row), the values for the attention add up to more than one. This is because we haven't applied the softmax function. Let's see what I mean, below are the sums at row level (note they don't necessarily add up to 1.0). I'm only reshaping the tensor so that each sum is at the same position as the row it represents in the above matrix."
      ],
      "metadata": {
        "id": "HO1BEE3yll8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(att_before_softmax, dim=1).reshape(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f36D2WxYlXPE",
        "outputId": "d217f66f-7266-46fa-b87d-26ffb8b01ff8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.3957],\n",
              "        [ 0.2466],\n",
              "        [-0.0571]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_matrix = torch.softmax(att_before_softmax, dim=1)\n",
        "\n",
        "attn_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3v6G_MQlZae",
        "outputId": "dfda7d1b-6641-4482-8bb7-a27ebd07aabe"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4043, 0.2605, 0.3352],\n",
              "        [0.2601, 0.3663, 0.3736],\n",
              "        [0.4168, 0.2690, 0.3142]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check that the rows add up to 1 now. Remember the reshape is just to align the rows in the attn_matrix above to the output of the cell below, but it's not doing anything to the values themselves"
      ],
      "metadata": {
        "id": "VVucaXabnDa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(attn_matrix, dim=1).reshape(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFZlLcddmdxJ",
        "outputId": "ee96eb67-2877-47d3-cdd9-a1148b15ea65"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [1.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This attention matrix (attn_matrix) is applied to the matrix V, and this is the actual context aware \"attention\" our model would be paying to each token in the embeding space, projected into the V space."
      ],
      "metadata": {
        "id": "ZzPtCxrTnWrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLE8Ovsjq9n_",
        "outputId": "2ee70165-c7bb-4279-b13c-ab285cc9ee12"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4043, 0.2605, 0.3352],\n",
              "        [0.2601, 0.3663, 0.3736],\n",
              "        [0.4168, 0.2690, 0.3142]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKrPXq_fpYHc",
        "outputId": "1db5de75-4dcc-41fc-a112-166de79fa1c7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7467,  0.0360, -0.5847,  0.0370],\n",
              "        [ 1.5090,  0.8009,  0.7957,  0.0151],\n",
              "        [ 0.4343,  0.9003,  0.3387,  0.2031]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = torch.mm(attn_matrix, V)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmDnrTz1my8X",
        "outputId": "300ebb43-22b6-4336-c822-df0b8ae33490"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8405, 0.5250, 0.0844, 0.0870],\n",
              "        [0.9092, 0.6391, 0.2659, 0.0910],\n",
              "        [0.8536, 0.5133, 0.0768, 0.0833]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output matrix shows the elements of \"V\" our model is going to pay attention to. The model at this stage is just providing \"relevance\" or context awareness to the semantic meaning of the words. It's enriching the text by combining context + meaning in the V projection space!"
      ],
      "metadata": {
        "id": "v82UED4qqW3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying this to BERT"
      ],
      "metadata": {
        "id": "5nu_U-RZE7qI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi Head Attention\n",
        "\n",
        "We discussed the attention mechanism in general, but now we will go over how this affects a real model like BERT. Most transformer models introduce a concept called \"Multi-Head-Attention\".\n",
        "\n",
        "In the attention mechanism, the concept of “heads” refers to the number of times the attention operation is performed in parallel. Each head computes a different weighted average of the values, allowing the model to attend to different parts of the input for each position in the output.\n",
        "\n",
        "The number of heads is a hyperparameter that can be tuned for a specific task. In general, using more heads allows the model to capture more complex relationships between the input and output, but it also increases the computational cost and memory usage.\n",
        "\n",
        "The number of heads is usually chosen based on empirical results on a validation set. For example, the original BERT paper used 12 heads for the bert-base model and 16 heads for the bert-large model.\n",
        "\n",
        "This is part of the beauty of attention. This step is extremely parallelizable! Each attention matrix can be calculated in parallel, making this process very friendly to run on GPUs."
      ],
      "metadata": {
        "id": "E5kLMdueJV7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizers\n",
        "\n",
        "There is also the idea of tokenizers. Tokenization is a crucial preprocessing step when working with large language models like BERT. Tokenization is the process of converting text into a sequence of tokens that can be fed into the model. A tokenizer is responsible for performing this conversion.\n",
        "\n",
        "Tokenizers are important for several reasons. First, they allow the model to handle input text of arbitrary length by breaking it down into smaller, fixed-size units (tokens). This is necessary because neural networks require fixed-size inputs.\n",
        "\n",
        "Second, tokenizers can greatly reduce the size of the model’s vocabulary. Instead of representing each word as a separate token, tokenizers can split words into subword units or characters. This allows the model to handle rare and out-of-vocabulary words by representing them as a sequence of smaller units.\n",
        "\n",
        "We will use the tokenizer provided by HuggingFace that uses the same scheme as the original BERT tokenizer did. So we will convert the text into a sequence of tokens that comprise the text, and then into a sequence of integers that represents the token ids. We can easily switch between integer representations of the tokens and the actual tokens as shown below."
      ],
      "metadata": {
        "id": "YtTDD9zFJZJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "vl4bKRqfKMzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am a dog\"\n",
        "\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N14i6H1IKOt4",
        "outputId": "44eee651-d3cf-4521-e56d-4419b7a3e2ee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 101, 1045, 2572, 1037, 3899,  102]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(input_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqgZd1UIKZ2k",
        "outputId": "688f10d5-aaa9-4ff4-b858-ee57fa50431e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'i', 'am', 'a', 'dog', '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick note:** Most tokenizers add some kind of separation token, generally  \"opening\" and \"closing\" tokens to indicate where the sentences start and stop. This is what the [CLS] and [SEP] tokens represent here.\n",
        "\n",
        "The [SEP] token indicates the separation between parts of the text and is used when there are multiple interactions, which can happen in conversations for example. In this case, it indicates the end of the text.\n",
        "\n",
        "The [CLS] token indicates the start of the sentence. It has been shown that the final hidden state corresponding to this token can be used as the aggregate representation of the entire sequence. This representation can then be fed into a classifier to make a prediction."
      ],
      "metadata": {
        "id": "5EGaQmbdKl7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
        "\n",
        "# tokenize the text\n",
        "text = \"I am a dog\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "with torch.inference_mode():  # I prefer inference_mode over torch.no_grad()\n",
        "    output = model(input_ids)\n",
        "\n",
        "# getting the attention matrix for the first layer\n",
        "attention_matrix = output[2][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvwuPynuE9yG",
        "outputId": "80653f18-7741-4047-ac12-e51cec76487c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkUIxMoJNge3",
        "outputId": "c7db9ba7-aad9-4a27-9472-ae5ed1409223"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 12, 6, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Attention\n",
        "\n",
        "In order to visualize attention we will have to look at these attention heads. Each attention head outputs an attention matrix for the input we pass. Some attention matrices will be looking at some relationships between words and other attention matrices will extract or \"attend to\" different relationships in the input data.\n",
        "\n",
        "One way to visualize the attention that our model is paying to the tokens and the relationship between them is by averaging the attention heads. If you notice the shape of the attention matrix, it has 12 heads of 6x6. Our encoded text has length 6. Basically, the attention matrix is actually a stack of 12 attention matrices, each of size 6x6. If we ignore the [CLS] and [SEP] tokens, we can visualize how the model is attending to the words in the text, as shown below:"
      ],
      "metadata": {
        "id": "TG-617NXM7wV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# avg attention weights across attention heads\n",
        "attention = attention_matrix.mean(dim=1).squeeze(0)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "# clip attention so it doesn't include the CLS and SEP tokns:\n",
        "num_tokens = len(tokens) - 2 + 1\n",
        "word_attention = attention[1:num_tokens, 1:num_tokens]\n",
        "word_input_ids = tokens[1:num_tokens]\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(word_attention, xticklabels=word_input_ids, yticklabels=word_input_ids, annot=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "-ryWAepMFv6X",
        "outputId": "21a0e077-a9c0-46fa-ad54-25ed2750c7cd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x600 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAH+CAYAAACRLHTCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABX3ElEQVR4nO3deVhUVQMG8HfYQdmHTVwAF1yREkVT0xR3zUxNzdLI3HfMhVTILdwzlzI191y/TLMSLRRXXFFQURRFEZEdREFZZub7gxobHRwYGeBe3t/33OeRM+eeObf5gMN7zrlXolAoFCAiIiISEb3y7gARERFRaeMAh4iIiESHAxwiIiISHQ5wiIiISHQ4wCEiIiLR4QCHiIiIRIcDHCIiIhIdDnCIiIhIdDjAISIiItHhAIeIiIhEhwMcIiIieiNr1qyBi4sLTExM4O3tjfPnzxdZd9++ffDy8oKVlRWqVKkCT09PbNu2TaXOZ599BolEonJ07dq1RH0y0OpKiIiIiADs3r0bfn5+WLt2Lby9vbFixQp06dIF0dHRsLe3f6W+jY0NZs6cifr168PIyAi///47fH19YW9vjy5duijrde3aFZs2bVJ+bWxsXKJ+SfiwTSIiItKWt7c3mjdvjtWrVwMA5HI5atSogfHjx2PGjBnFauPtt99Gjx49MG/ePACFCU5mZib279+vdb8qTIIzr9bg8u4ClcCkHunl3QUqIesNkeXdBSqBj5xalHcXqIR23P+1zN4rP/WuztqWmzsjNzdXpczY2FhtgpKXl4dLly7B399fWaanpwcfHx+EhYVpfC+FQoGjR48iOjoaixYtUnktNDQU9vb2sLa2RocOHTB//nzY2toW+zq4BoeIiIiUgoKCYGlpqXIEBQWprZuamgqZTAYHBweVcgcHByQmJhb5Ho8fP0bVqlVhZGSEHj16YNWqVejUqZPy9a5du2Lr1q0ICQnBokWLcPz4cXTr1g0ymazY11FhEhwiIiIqJnnxf9GXlL+/P/z8/FTKSrr+RRNzc3NcuXIFT58+RUhICPz8/ODm5ob27dsDAAYOHKis26RJE3h4eKB27doIDQ1Fx44di/UeHOAQERGRUlHTUepIpVLo6+sjKSlJpTwpKQmOjo5Fnqenp4c6deoAADw9PXHjxg0EBQUpBzgvc3Nzg1QqRUxMTLEHOJyiIiIiEhqFXHdHCRgZGaFZs2YICQlRlsnlcoSEhKBVq1bFbkcul7+y7ue/4uPjkZaWBicnp2K3yQSHiIiItObn54ehQ4fCy8sLLVq0wIoVK5CdnQ1fX18AwJAhQ+Ds7KxcxxMUFAQvLy/Url0bubm5+PPPP7Ft2zb88MMPAICnT59izpw56Nu3LxwdHXHnzh1MmzYNderUUdlGrgkHOEREREIjL1nSoksDBgxASkoKAgICkJiYCE9PTwQHBysXHsfFxUFP78WEUXZ2NsaMGYP4+HiYmpqifv362L59OwYMGAAA0NfXR2RkJLZs2YLMzExUq1YNnTt3xrx580q0FqjC3AeH28SFhdvEhYfbxIWF28SFpyy3ieclXNdZ20bVGums7bLENThEREQkOpyiIiIiEpoKNEVVUTHBISIiItFhgkNERCQ0JdzOXRkxwSEiIiLRYYJDREQkNDp8VINYMMEhIiIi0WGCQ0REJDRcg6MRExwiIiISHSY4REREQsP74GjEAQ4REZHAKDhFpRGnqIiIiEh0mOAQEREJDaeoNGKCQ0RERKLDBIeIiEhouAZHIyY4REREJDpMcIiIiISGj2rQiAkOERERiQ4THCIiIqHhGhyNOMAhIiISGm4T14hTVERERCQ6THCIiIiEhlNUGjHBISIiItFhgkNERCQ0XIOjERMcIiIiEh0mOERERAKjUPBGf5owwSEiIiLRYYJDREQkNNxFpREHOERERELDRcYacYqKiIiIRIcJDhERkdBwikojJjhEREQkOkxwiIiIhEbObeKaMMEhIiIi0WGCQ0REJDRcg6MRExwiIiISHSY4REREQsP74GjEAQ4REZHQcIpKI05RERERkegwwSEiIhIaTlFpxASHiIiIRIcJDhERkdAwwdGICQ4RERGJDhMcIiIigVEo+KgGTZjgEBERkegwwSkBryGd0GpED1S1s0TSjTgEB25BQsRdtXXt6jqj3ZR+cGrsCqsadjg8ZxvObwx+pZ65gzU6+g9E7fZNYWhqjIx7Sfjtyx/x6Gqsri+nUjB8tyeMOvWDxMIa8vi7eL7nB8jv31Jft3VXGHh3hH61WgAAWVwMcg9sfqW+nmMNGH/wOfTrNgH09CFPjMOzdfOhyEjR+fWIzehRQzHFbzQcHe0QGRmFiZNm48LFK0XW79u3J+Z8PRUutarjdkwsvvrqGxwKPqpSp379Ogj6ZibebdsSBgYGiLpxCx8NGI4HDxIAACF/7UW7du+onPPjum0YO25GqV9fZdBpSDf0HPEBLO2sEHfjHrYEbsCdiNtq6zrXrYH+UwbBtXFt2NWwx9Y5PyF44+8qdXw+6QKfT7pCWt0eAPDw9gPs+24PIkLDdX4tgsI1OBoxwSmmhj1botOswTjx3T6s7zkLSTfi8PG2GTCztVBb38DUGBlxyTi6aBeeJGeorWNiYYbPfgmELF+GnUMXY63PNPw1/2c8f5yty0upNAyavQvjviOQ+8fPyAkaD9nDWJiNnw9JVUu19fXreqDgYihyVsxAzhI/KDJSYDZ+ASSWtso6EqkTzPyWQp70ADnfTkf2gjHI/XMHkJ9XVpclGv37v4+lSwIxb/5yNPfuiojIKPz5x8+ws7NVW79VSy/8vG0NNm3aCa8WXfDbb4fxy/9+QqNG7so6bm61cPzYfkRHx6Bjp354q5kPFnyzAs+f56q0tX7DdjjX8FQeM/zn6/Raxaplz9b4ZJYv9n23GzN7TkHcjXuYsS0AFrbqv8eMTY2RHJeEXYu2ISM5XW2d9Edp2LVoG2b1/BKzek3F9TNXMWX9DDjXraHLSxEehVx3h0hwgFNMLb/ohsu7jiFi7wmk3n6IP77aiPxnufD8qJ3a+o8i7yLkm524fvAsZLkFauu8M7oXsh6l4eDUdUiIuIvMBym4e/IqMuKSdXkplYZRhz7IP30IBWf/gjwxDrk7V0GRlwvDdzqrrf9882Lkn/gD8vi7kCfF4/n27wCJHvTreyrrGL8/FAXXLyD3142Qx9+BIvURZFfPQfH0cRldlXhMnjgcG37agS1b9+DGjdsYM3YGcnKewfezgWrrjx8/DIcPh2LZ8rW4eTMGgV8vweXL1zBmtK+yzry503Eo+Chm+C/AlSvXcffuffz++19ISUlTaSsn5zmSklKUx5MnT3V6rWLV/Yv3cWzXXzi+9yge3o7HT1+tRe6zXLT7qKPa+ncjY7Djmy0IO3gKBUX8XAwPuYgrx8KReO8REmMTsGfJz3ie8xx1366ny0shEeIApxj0DPXh1MQVsaeuvShUKBB76hqqv11X63brdWqGhMhY9P1+AvwufY/hfy7AWwPfK4UeE/QNoFezLmTRV16UKRSQ3bwCPdcGxWvDyBjQ14ci+0nh1xIJDBo3hzz5IUzHzUeVRTthNvVbGDRtVerdFztDQ0O8/bYHQo6eVJYpFAqEHD2Fli2bqT2npXczlfoAcOSvUGV9iUSC7t064vbtu/jz95+REB+BM6cO4v33u7zS1seD+iAx4SquXA7BgvkzYGpqUopXVznoGxrAtUltXDsVoSxTKBS4dioSdd92f82ZxSfR00OrXm1gbGqC2+HRpdKmaMjlujtEolzW4OTm5iI3VzUyLlDIYCDRL4/uaGRmbQ49A308TVX9Kz07NQvS2tW0bte6hh28PumIsxsO4fSaA3DycEOXOUMgyy9A5C8nNTdARZJUtYBEXx/yLNXpQcWTDOg7VC9WG8Z9PoficTpkNy8XtmluBYmJGYw6f4Tcg1sg278RBg2bwWT4LDz7bgZkt6+W+nWIlVRqAwMDAyQnpaqUJyenoL57bbXnODraISlZdZ1TUlIqHB3sAAD29lKYm1fFtKljERC4GP4zv0GXzu3xvz0b4NOpP06cPAsA2LlrP+Li4pHwKAlNmjRA0IKZqFevNvp/NFwHVype5tbm0DfQx+OXfi4+Ts1EtdrOb9R2DfeamPPrQhgaG+F59nN8O3IhHt6Of6M2qfIplwFOUFAQ5syZo1LW3qIxOlh5lEd3yo1ETw8JV+/i2JI9AIDE6/dh714DzT7pyAFOOTPq3B+GzdohZ8U0oCC/sFAiAQAURIYh/+h+AEBe/F3ouzWEYZvuHOCUMz29wkD6t4OH8d3K9QCAiIjraNXKCyNGfKoc4Gz46WflOdeu3UTio2T8dWQP3Nxq4e7d+2XfcXpFwt0E+Hfzg5m5GVp0fwejlk3AvAGzOMj5LxGtldGVcpmi8vf3x+PHj1WOdy0blUdXiiUn4wnkBTJUlaounKsitcDTFO3XXjxJzkTq7YcqZakxD2FRTf0iSyo+xdMsKGQy6FlYq5RLzK1fSXVeZujTF0adP0LOqpmQP7z3UpsFkD+KU6kvS3wAPRu7Uut7ZZCamo6CggLYO0hVyu3t7ZCYpH43WmJiChzsVf87OzhIlfVTU9ORn5+PGzdUd/DcvHkbNWsUnSicO1+4O6dObZeSXkal9iTjCWQFMli+9HPRUmqFzJTMN2pbll+ApPuJiL12F7sXb0fcjXvo6tvzjdqkyqfYCY6fnx/mzZuHKlWqwM/P77V1ly9f/trXjY2NYWxsrNqRCjo9BQDyfBkeXY2FS+tGiD5yqbBQIoFr68a4sOWI1u3GX7oFWzcnlTIbVyc8fphaxBlUbLICyONuQ9/dEwURYYVlEgn03T2Rf/y3Ik8z6tQPRl0HImfVLMjjXtrqKiuA/P4t6L00xaVn7wx5OheGl0R+fj7CwyPR4b02+O23wwAK19B0eK8Nvv9hk9pzzp67hA4d2mDlqg3KMp+O7+Ls2UvKNi9ejEC9eqpTXHXruuF+XNF/+Xs2Lfzj6lEiP8OSkOUXIPbqHTRq7YGLR84DKPwMG7VugiNbDpXqe0n09GBgZFiqbQqeiNbK6EqxBziXL19Gfn6+8t9FkfwT44vN2Q2H0HvZSDyKjEVCxB20+LwrDM2MEbH3OACg9/JReJKYgaOLdwMoXJhsV7fwF6G+kQHMHa3h0LAW8rKfI+N+krJN332BaD32fUT9fg7OnrXx9sfv4Q//n8rnIkUm7+ivMBkyBbL7tyG/Hw3D9z6AxNgY+WF/AQBMhk6BPDMNeQc2AwCMOvWHUc9P8XzTIijSkyD5J/1R5D4Dcp8XtvnXLzAZNgOGMddQcCsCBg29YNDEG89WTC+XaxSyb79bj00/fYtL4ZG4cOEyJowfjipVTLF5S+H30KaN3yEh4RFmzloIAFi16iccDfkfJk8aiT8P/Y0BH/VGs2YeGDVmmrLNpct/wM6ff8DJk2cRevwMunRuj549OqGjTz8AhdvIBw3sg0OHQpCWnoEmTRpg2ZKvceJEGK5evVH2/xEE7s8Nv2HUsgm4G3kHdyJuo9vnPWFiZoLje0MAAKOXT0B6Yjp2L94OoHBhcvV/fi4aGBnAxtEWtRq64Hn2cyTdTwQADJj2CSJCw5GakALTKqZ4p/e7aNCyERZ+Ord8LpIEq9gDnGPHjqn9d2UR9ftZmNmao51fv8Ib/UXdx44hi5CdmgUAsKhmC4Vcoaxv7mCNEYe+UX79zsieeGdkT9wLi8K2gQsAFG4l3ztiBTpMH4B3J/RBZnwKjszZjmv7z5TtxYlUwaUTyK1qCeOen0BiYQN5/B3krJ4NxZNMAIDE2h56//nMDN/tAYmhIUxHzFJpJ/eP7cj7o3DdRkHEGTzfuRrGXT6Ccf9RhdvJ18+H7M71Mrsusdi79zfYSW3wdcCXcHS0Q0TEdfTo+QmSkwsTzJo1qkH+n79Sw85exCdDxmHunGmYP286bsfEom+/Ybh+/cXumgMHgjFm7AxMnzYeK76di+hbd9F/wHCcPnMBAJCXl4+OHdpgwvgvUKWKKR48eIRf9/+JBd98V7YXLxJnfz8NC1sL9PMbCCs7a9yPisXCIXOR9c/CY9tqdpD/53vM2sEaQYe+VX7dc+QH6DnyA0SFXcP8gbMBABZSS4xePhFW9tbIeZKDBzfvYeGnc1V2axG4BqcYJAqFQqG5mu7NqzW4vLtAJTCph/qbdFHFZb0hsry7QCXwkVOL8u4CldCO+7+W2Xs9O7RSZ22bdpugs7bLEu+DQ0RERKLDZ1EREREJDRcZa8QEh4iIiESHCQ4REZHQcJGxRkxwiIiISHSY4BAREQkN1+BoxASHiIiIRIcJDhERkdBwDY5GHOAQEREJDaeoNOIUFREREYkOExwiIiKh4RSVRkxwiIiISHSY4BAREQkN1+BoxASHiIiIRIcDHCIiIqGRy3V3aGHNmjVwcXGBiYkJvL29cf78+SLr7tu3D15eXrCyskKVKlXg6emJbdu2qdRRKBQICAiAk5MTTE1N4ePjg9u3b5eoTxzgEBERkdZ2794NPz8/BAYGIjw8HE2bNkWXLl2QnJystr6NjQ1mzpyJsLAwREZGwtfXF76+vjh8+LCyzuLFi7Fy5UqsXbsW586dQ5UqVdClSxc8f/682P3iAIeIiEhoFArdHSW0fPlyDB8+HL6+vmjYsCHWrl0LMzMzbNy4UW399u3bo0+fPmjQoAFq166NiRMnwsPDA6dOnfrn0hRYsWIFZs2ahd69e8PDwwNbt25FQkIC9u/fX+x+cYBDREQkNDqcosrNzUVWVpbKkZubq7YbeXl5uHTpEnx8fJRlenp68PHxQVhYmMbLUCgUCAkJQXR0NN59910AQGxsLBITE1XatLS0hLe3d7HaVPaj2DWJiIhI9IKCgmBpaalyBAUFqa2bmpoKmUwGBwcHlXIHBwckJiYW+R6PHz9G1apVYWRkhB49emDVqlXo1KkTACjPK2mbL+M2cSIiIqHR4TZxf/9Z8PPzUykzNjYu1fcwNzfHlStX8PTpU4SEhMDPzw9ubm5o3759qb0HBzhERESkZGxsXOwBjVQqhb6+PpKSklTKk5KS4OjoWOR5enp6qFOnDgDA09MTN27cQFBQENq3b688LykpCU5OTiptenp6Fvs6OEVFREQkNAq57o4SMDIyQrNmzRASEqIsk8vlCAkJQatWrYrdjvyftT8A4OrqCkdHR5U2s7KycO7cuRK1yQSHiIiItObn54ehQ4fCy8sLLVq0wIoVK5CdnQ1fX18AwJAhQ+Ds7KxcxxMUFAQvLy/Url0bubm5+PPPP7Ft2zb88MMPAACJRIJJkyZh/vz5qFu3LlxdXTF79mxUq1YNH3zwQbH7xQEOERGR0FSgRzUMGDAAKSkpCAgIQGJiIjw9PREcHKxcJBwXFwc9vRcTRtnZ2RgzZgzi4+NhamqK+vXrY/v27RgwYICyzrRp05CdnY0RI0YgMzMTbdq0QXBwMExMTIrdL4lCocWmdx2YV2tweXeBSmBSj/Ty7gKVkPWGyPLuApXAR04tyrsLVEI77v9aZu/1bKu/zto2HaJ+x5TQMMEhIiISmoqRTVRoXGRMREREosMEh4iISGgq0BqciooDHCIiIqHhAEcjTlERERGR6DDBISIiEpoS3pCvMmKCQ0RERKLDBIeIiEhgFHJuE9eECQ4RERGJDhMcIiIioeEuKo2Y4BAREZHoMMEhIiISGu6i0ogDHCIiIqHhImONOEVFREREosMEh4iISGi4yFgjJjhEREQkOkxwiIiIhIYJjkZMcIiIiEh0mOAQEREJjYK7qDRhgkNERESiwwSHiIhIaLgGRyMOcIiIiISGN/rTiFNUREREJDpMcIiIiISGz6LSiAkOERERiQ4THCIiIqHhGhyNmOAQERGR6FSYBGfGpXnl3QUqAauaHcq7C1RCDW1qlncXqATek1Up7y5QBabgNnGNmOAQERGR6FSYBIeIiIiKiWtwNOIAh4iISGi4TVwjTlERERGR6DDBISIiEhpOUWnEBIeIiIhEhwkOERGR0HCbuEZMcIiIiEh0mOAQEREJDdfgaMQEh4iIiESHCQ4REZHQ8D44GnGAQ0REJDScotKIU1REREQkOkxwiIiIBIZPE9eMCQ4RERGJDhMcIiIioeEaHI2Y4BAREZHoMMEhIiISGiY4GjHBISIiItFhgkNERCQ0vNGfRhzgEBERCQ2nqDTiFBURERGJDhMcIiIigVEwwdGICQ4RERGJDhMcIiIioWGCoxETHCIiIhIdJjhERERCw4dtasQEh4iIiESHCQ4REZHQcA2ORhzgEBERCQ0HOBpxioqIiIhEhwkOERGRwCgUTHA0YYJDREREosMEh4iISGi4BkcjJjhEREQkOkxwiIiIhIYJjkZMcIiIiEh0mOAQEREJjIIJjkYc4BAREQkNBzgacYqKiIiIRIcJDhERkdDwYeIaMcEhIiIi0eEAh4iISGAUcoXODm2sWbMGLi4uMDExgbe3N86fP19k3fXr16Nt27awtraGtbU1fHx8Xqn/2WefQSKRqBxdu3YtUZ84wCEiIiKt7d69G35+fggMDER4eDiaNm2KLl26IDk5WW390NBQDBo0CMeOHUNYWBhq1KiBzp074+HDhyr1unbtikePHimPnTt3lqhfHOAQEREJjVyhu6OEli9fjuHDh8PX1xcNGzbE2rVrYWZmho0bN6qt//PPP2PMmDHw9PRE/fr1sWHDBsjlcoSEhKjUMzY2hqOjo/KwtrYuUb84wCEiIiKl3NxcZGVlqRy5ublq6+bl5eHSpUvw8fFRlunp6cHHxwdhYWHFer+cnBzk5+fDxsZGpTw0NBT29vZwd3fH6NGjkZaWVqLr4ACHiIhIaOS6O4KCgmBpaalyBAUFqe1GamoqZDIZHBwcVModHByQmJhYrEuZPn06qlWrpjJI6tq1K7Zu3YqQkBAsWrQIx48fR7du3SCTyYrVJsBt4kRERPQf/v7+8PPzUykzNjbWyXstXLgQu3btQmhoKExMTJTlAwcOVP67SZMm8PDwQO3atREaGoqOHTsWq20OcIiIiARGl49qMDY2LvaARiqVQl9fH0lJSSrlSUlJcHR0fO25S5cuxcKFC/H333/Dw8PjtXXd3NwglUoRExNT7AGOVlNUaWlpGDt2LBo2bAipVAobGxuVg4iIiHRIh1NUJWFkZIRmzZqpLBD+d8Fwq1atijxv8eLFmDdvHoKDg+Hl5aXxfeLj45GWlgYnJ6di902rBOfTTz9FTEwMhg0bBgcHB0gkEm2aISIiIoHz8/PD0KFD4eXlhRYtWmDFihXIzs6Gr68vAGDIkCFwdnZWruNZtGgRAgICsGPHDri4uCjX6lStWhVVq1bF06dPMWfOHPTt2xeOjo64c+cOpk2bhjp16qBLly7F7pdWA5yTJ0/i1KlTaNq0qTanC9rOXw5i047/ITU9A+513PDV5NFo0tBdbd2/Qk9j/dbdePAwAQUFBahZ3RlDB32I97u+iNdmzl+GA4f+VjmvtXcz/Lh8vk6vQ6xGjPwUkyaNhIODHa5evYEpUwJx6WJEkfX79OmO2QFTUKtWddyJicXs2Qtx+HCo8vXsnHtqz5v51TdYsWIdatasjhn+49Gu3TtwcLDDo0dJ2LVrPxYvWo38/PxSvjrxGeDbF5+NGQypnQ1uRcUgaOZyXLscVWT9Tr06YNy0EahWwxFxsfH4dv4anAp5sVPDRmqNybPHolW7FjC3MEf42SsImrkMcbHxyjrVazljSuB4vOXtASMjI5w+dhZBXy1DemqGTq9VrBoO9YHHqB4wtbNE+o04nJm9FSlX7qqta13PGc2+7AtpE1eY17BDWOA2XPvpsEqdgWHfwryG3SvnXt/8F87M2qKTaxCiivQ08QEDBiAlJQUBAQFITEyEp6cngoODlQuP4+LioKf3YsLohx9+QF5eHvr166fSTmBgIL7++mvo6+sjMjISW7ZsQWZmJqpVq4bOnTtj3rx5JVoLpNUAp379+nj27Jk2pwraob+PY/GqdQiYOh4eDd2xbc9+jPSbhYM718PW2uqV+pYW5hgxdABca9WAoYEBjp85j9nfLIettRVaezdT1mvT0gvzv5qs/NrQ0LAsLkd0+vbtiYULZ2HihFm4cOEyxo77HAcObMVbnh2QkvLq9kJv77exectKBAYsxqFDIfhoQG/s2r0Ord/piaioWwAAN9fmKud07twe3/+wCPv3HwIAuLvXhp6eHiaM/wp37txDw0buWLM6CFXMTPHVV9/o/qIFrEvvjpj69QTMm74YV8Ov45PhA7B257d4v81AtYONpl5NsOiHOVj5zVoc/+sUuvfpgu82LcKAzp8h5mbhL9TvNi9CQX4BJn42HdlPsvHpyEFYt3cl+rz7MZ7lPIepmQl+3L0C0ddjMLzveADA2OnDsWrbUnzS/QsoFBXnl4YQuPXyRsuAwTjlvwnJl2PQ+Iuu6LZ9Ova0m4rnaVmv1Nc3NUZWXAru/n4erQI/Udvm/h4BkOi/+GVo7V4dPXb5I/aPou+MS+Vv3LhxGDdunNrXQkNDVb6+d+/ea9syNTXF4cOHX1unOCQKLb6jL1y4gBkzZiAgIACNGzd+5ReyhYVFiTuSn6p+xF+RDBo+CY3r18PMKWMAFM4z+vQZgo/7vY8vPv2oWG309x2Hd1u1wPgRQwAUJjhPnmZj5cIAnfVbF6xqdijvLrwi9Ph+XLoUgSl+gQAAiUSCW7fDsPaHLVi27IdX6m/ZuhpVqpiiX99hyrJjob8iMjIKEyfMVPseu3avg3nVKujRY3CR/Zg0aQS+GP4JGjd69w2vqHTVtij+3HVZ+PnPDbh25QaCvloGoPDzOhJ+ADt/2ouNq7e9Un/xj/NgamaK8Z9+qSzb/sd63Lx2G/OnL0Yttxo4eGYP+rT7GHeiY5VtHrv6O1Z+sxb7dhxEq3Yt8P2O5Wjj3hnZT3MAAFXNq+BU9BGMHDAJ505eKIMrL57xBrXLuwsa9T74NVIi7uLMrK2FBRIJPr7wHa5v+gsRaw6+9tyBYd/i2obgVxKcl7X8+hPU9HkLe9pMKa1u68zw+O1l9l7pvdvprG2bA8d11nZZ0mqRsZWVFbKystChQwfY29srnydhZWVV4jsNCkV+fj6iom+jZXNPZZmenh5aenki4toNjecrFAqcvXgZ9+Li0cyzscprFy5H4t0eA9Fz4BeYu2QVMh+/+pcPvZ6hoSHeeqsxjh07rSxTKBQ4dvQ0Wni/rfYcb++3cOzoaZWyv/8+Ae8W6uvb20vRtet72LJl92v7YmFpjoyMzJJdQCVjYGiABh7uOHvixYBCoVDg3MkLaOrVWO05TZs1xrkTqgOQM6HnlPWNjIwAALnP81TazMvNx1veTZV1FAoF8vJeTB/m5uZBLpfjbe/X7+IgVXqG+pA2ccXDk9dfFCoUeHjyOuzfrlNq71H3w9a4tUscv3CpbGk1RTV48GAYGhpix44dlWaRcUZmFmQyOWxtVAdwtjbWiI2LL+Is4MnTbHT44BPk5+VDT18Ps6aMxTv/+QXaumUz+LRrDedqDnjw8BG++3EzRk2ZjZ9/XA59fX2dXY/Y2EqtYWBggOSkVJXy5OQU1HNX/5ewg4MdkpNfre/gIFVbf/DgvnjyJBsHDhT9F6ebWy2MGjWU01MaWNtYwcDAAGkp6SrlaSnpcK1TS+05UntbtfWl9rYAgNiYe0iIf4SJM0dj7tRFeJbzDJ+OHAhHZwdlncjwa3iW8xyTZ43FyqAfIJFIMHHmGBgYGEBqr/5zJ/VMbMyhZ6CPZymPVcqfpT6GVZ3SSQtdunjByMIMt/aeKJX2xERRwt1OlZFWA5xr167h8uXLcHdXv7hWk9zc3Fdu+6yXm6uzGwmVpypmpvhl8xrk5DzD2UtXsGTVelSv5oQWbxf+tdjdp72ybr3arqhX2xXdPvocFy5HoqXXW+XUa1Ln0yEfYffu/UXestypmgP2H9iCX3/9E5s37Srj3lFBgQyTP/fHnOVf4XT0ERQUFODciYs4GXJG+UdYRlomvhw+E7MWTcXHX/SHXC7HoV//QlTETSj4G6PCcR/YDg+ORSAnKbO8u0ICpNUAx8vLCw8ePNB6gBMUFIQ5c+aolM2aOgEB0yZq1V5ZsLaygL6+HtLSVRc/pqVnQGpT9LScnp4ealavBgCoX6827t57gA3bdisHOC+r4ewEaysLxMU/4gCnBNJSM1BQUAD7l9IXe3s7JCWlqD0nKSkF9vbq6qe+Uvedd5rD3b02hg5Rv4jO0ckehw7txLmzlzBurL+WV1F5ZKRnoqCgALZ2qvfNsrWzQWqy+ufNpCanaax/IzIaH/kMRVXzKjA0MkRGWiZ+/nMDrkfcVNYJO34ePVr2h5WNJWQFMjzJeoqjkb8j/kBCKV6h+D1PfwJ5gQymdpYq5aZSS+QkPy7irOKr6myLam0b4+/hK964LVHieFwjrdbgjB8/HhMnTsTmzZtx6dIlREZGqhya+Pv74/HjxyrH9ImjtOlKmTE0NERD97o4d/GKskwul+PcpSto2rhBsduRK+TIe8324cTkFGQ+fgI7W94wsSTy8/Nx+fI1tG//jrJMIpGg/Xvv4Py5cLXnnDt3Ge3fe0elrEOHNjh3/tX6Q4cOQHh4JK5efXW9lVM1BwQH78KVy9cwcuRU7sQphoL8AtyIjIZ32xc3+JJIJPBu44WIi9fUnhNx6ZpKfQBo+W4LtfWfPslGRlomarpWR8Om9XEs+NUpjsz0x3iS9RQtWjeDjdQaoYdPvuFVVS7yfBlSr8bCuU2jF4USCaq1aYTk8Jg3br/egHZ4npqFuJArb9yWGCnkujvEQqsEZ8CAAQCAzz///JXXJBKJxodhqbsNdH7eq381VzRDBvTBzAXL0Kh+XTRu6I7te/bj2fNcfNCjEwDAf95S2EttMXl04c2N1m/djUb166KGsxPy8vNxMuwCfg8+illfFqYAOTnP8P3Gn9GpfWtIbW3w4GECln+/ETWrV0PrIhbGUtFWrdyAdeuX4XL4VVy8eAVjxw2DmZkZtm3bCwBYv34ZEhKSEBi4GADw/ZqNOHxkNyZM+ALBwcfQr38vvP12E4wfp5rAmJtXRZ8Pu8Pff8Er7/nv4OZB3EP4f7UAdna2yteKSo6o0NYfd2L+d7MRFXETVy9fxyfDB8LUzAT7d/0OAFiwKgBJj1Kw8pvCHXA/r9+Djb9+jyGjBuHE32fQ7QMfNGpaH3OnLlS22alXB2SkZeBRfBLqNqiN6fMn49ihEwg7/mKLce+BPRB76x7S0zLR1Ksxps+bjG3rduHenbiy/Q8gAlfXHUK7b0ciJSIWKVfuoPEXXWFoaoxbuwsXBbdfMRLZiRm4sHAPgMJFw1Z1nf/5twHMnGxg07AmCnJykXXvP7f6l0hQ76N3cet/J6GQieg3LpUprQY4sbGxpd0PQejm0w4ZmY+xesN2pKano37d2li7bJ5yiupRUjL0/rPg+tnz55i/bA2SklNhbGwE11o1EBQwFd18Crf36enr4dadWPx26G9kPc2GvdQG77R4G+OGD1HuCKHi++WX3yG1s8Gs2ZPh4GCHyMgb+OCDocqFxNVrOEP+n5tjnTsXDt/PJiIgcAq+njMVd2LuYeCAEcp74PyrX/9ekEgk2Lvnt1fes2OHtqhTxxV16rgiJuacymtVzFxK/yJF5PCBEFjbWmPMtC8gtbNF9PXbGD1osvIeOI7ODpDLX/xyi7h4FTPGBGL89BGY4D8KcbEPMNF3uvIeOABgZ2+LqV9PgK2dDVKSU3FwTzB+/Hajyvu61K6JiV+NhqWVBR4+eIT1323Gth+5Zkobdw+eg4mtBZp92RdmdpZIi7qPQ58uxrPUwp2gVZylKjekM3OwRt8jLxbgNx3VA01H9UBC2A380f/FHxDObRvBvLqUu6deh+M+jbS6D86/oqKiEBcXh7y8F9syJRIJevXqVeK2hHAfHHqhIt4Hh16vot0Hh15PCPfBIVVleR+c1C66uw+O9LA4BpZaJTh3795Fnz59cPXqVUgkEuWag393KmiaoiIiIiLtiWmtjK5otch44sSJcHV1RXJyMszMzHDt2jWcOHECXl5er9ySmYiIiKisaZXghIWF4ejRo5BKpdDT04O+vj7atGmDoKAgTJgwAZcvXy7tfhIREdE/mOBoplWCI5PJYG5uDgCQSqVISCi8f0StWrUQHR1der0jIiIi0oJWCU7jxo0REREBV1dXeHt7Y/HixTAyMsK6devg5uZW2n0kIiKi/2CCo5lWA5xZs2YhOzsbADB37lz07NkTbdu2ha2tLXbvfv2DCImIiOgNKcT/DMg3pdUAp0uXLsp/16lTBzdv3kR6ejqsra0rxYM3iYiIqGLTaoCjjo0NHy1ARERUFjhFpZlWi4yJiIiIKrJSS3CIiIiobCjkXA6iCRMcIiIiEh0mOERERALDNTiaMcEhIiIi0WGCQ0REJDAK3gdHIw5wiIiIBIZTVJpxioqIiIhEhwkOERGRwHCbuGZMcIiIiEh0mOAQEREJjEJR3j2o+JjgEBERkegwwSEiIhIYrsHRjAkOERERiQ4THCIiIoFhgqMZBzhEREQCw0XGmnGKioiIiESHCQ4REZHAcIpKMyY4REREJDpMcIiIiASGTxPXjAkOERERiQ4THCIiIoFRyMu7BxUfExwiIiISHSY4REREAiPnGhyNOMAhIiISGC4y1oxTVERERCQ6THCIiIgEhjf604wJDhEREYkOExwiIiKB4cM2NWOCQ0RERKLDBIeIiEhguAZHMyY4REREJDpMcIiIiASGN/rTjAMcIiIigeGN/jTjFBURERGJDhMcIiIigeE2cc2Y4BAREZHoMMEhIiISGC4y1owJDhEREYkOExwiIiKB4S4qzZjgEBERkegwwSEiIhIY7qLSjAMcIiIigeEiY804RUVERESiU2ESnC+9virvLlAJbLBqXd5doBKaL4sp7y5QCdTILyjvLlAFxkXGmjHBISIiItGpMAkOERERFQ/X4GjGBIeIiIhEhwkOERGRwHCXuGZMcIiIiEh0mOAQEREJDNfgaMYBDhERkcBwm7hmnKIiIiIi0eEAh4iISGDkOjy0sWbNGri4uMDExATe3t44f/58kXXXr1+Ptm3bwtraGtbW1vDx8XmlvkKhQEBAAJycnGBqagofHx/cvn27RH3iAIeIiIi0tnv3bvj5+SEwMBDh4eFo2rQpunTpguTkZLX1Q0NDMWjQIBw7dgxhYWGoUaMGOnfujIcPHyrrLF68GCtXrsTatWtx7tw5VKlSBV26dMHz58+L3S+JQlExnkk60WVgeXeBSqBlnn55d4FKiI9qEJZlcC3vLlAJdU3aVWbvdcKxv87afjdxb4nqe3t7o3nz5li9ejUAQC6Xo0aNGhg/fjxmzJih8XyZTAZra2usXr0aQ4YMgUKhQLVq1TBlyhR8+eWXAIDHjx/DwcEBmzdvxsCBxRsvMMEhIiIipdzcXGRlZakcubm5auvm5eXh0qVL8PHxUZbp6enBx8cHYWFhxXq/nJwc5Ofnw8bGBgAQGxuLxMRElTYtLS3h7e1d7DYBDnCIiIgER67Q3REUFARLS0uVIygoSG0/UlNTIZPJ4ODgoFLu4OCAxMTEYl3L9OnTUa1aNeWA5t/z3qRNgNvEiYiI6D/8/f3h5+enUmZsbKyT91q4cCF27dqF0NBQmJiYlGrbHOAQEREJjBy6uw+OsbFxsQc0UqkU+vr6SEpKUilPSkqCo6Pja89dunQpFi5ciL///hseHh7K8n/PS0pKgpOTk0qbnp6exbwKTlERERGRloyMjNCsWTOEhIQoy+RyOUJCQtCqVasiz1u8eDHmzZuH4OBgeHl5qbzm6uoKR0dHlTazsrJw7ty517b5MiY4REREAqPQYYJTUn5+fhg6dCi8vLzQokULrFixAtnZ2fD19QUADBkyBM7Ozsp1PIsWLUJAQAB27NgBFxcX5bqaqlWromrVqpBIJJg0aRLmz5+PunXrwtXVFbNnz0a1atXwwQcfFLtfHOAQEREJjLY35NOFAQMGICUlBQEBAUhMTISnpyeCg4OVi4Tj4uKgp/diwuiHH35AXl4e+vXrp9JOYGAgvv76awDAtGnTkJ2djREjRiAzMxNt2rRBcHBwidbp8D44pBXeB0d4eB8cYeF9cISnLO+D85fDAJ213Slpt87aLktMcIiIiASmIk1RVVRcZExERESiwwSHiIhIYCrSGpyKigkOERERiQ4THCIiIoFhgqMZExwiIiISHSY4REREAsNdVJpxgENERCQwco5vNOIUFREREYkOExwiIiKB0eXTxMWCCQ4RERGJDhMcIiIigakQD5Gs4JjgEBERkegwwSEiIhIY3uhPMyY4REREJDpMcIiIiARGLuEuKk04wCEiIhIYLjLWjFNUREREJDpMcIiIiASGi4w1Y4JDREREosMEh4iISGD4sE3NmOAQERGR6DDBISIiEhg+bFMzJjhEREQkOkxwiIiIBIb3wdGMAxwiIiKB4SJjzThFRURERKLDBIeIiEhgeKM/zZjgEBERkegwwSEiIhIYLjLWjAkOERERiQ4THCIiIoHhLirNmOAQERGR6DDBKYE2n3ZGh5G9YGFniYc34vBL4CbERdxRW9exbnV09+uP6k3cYFvdDvvmbsHxjYdU6nSd1A/dJvVTKUu68xDfdJyis2uobOp+1gn1R/eAqZ0lMqLicGnWFqRfuau2rkU9Z3hM7QdrD1dUrWGH8IBtiN4QrFJHoidB4yl94dK3NUzsrPAsKQOxe07g+or9ZXA14jPItx98xwyG1N4W0VG38c1Xy3D1clSR9Tv36oDx00fCuYYT7sc+wPJ5a3Ay5IzydVs7G/jNGot32nvD3MIcl85exoKvliEu9gEAoFoNJ/x1cb/atid/4Y8jB4+W6vVVBjV9O8N1TC8Y2VviSVQcbny1CY8vq/+5WNW9OupM6w9LDzeY1rTDjdlbcH+d6s9F/SomqDvjIzh0aw4jqSWyrt3DjVmbkVXE921lxV1UmjHBKaa3erZCn1mf4vB3/8OSHv5IiLqP0Vv9UdXWQm19I1MjpMYl4+CiHXicnFFku4+iH2BW85HK47t+X+voCiqfmu+3xFuBg3Ft+T4Ed5mFzKg4vLdjBoyL+MwMTI3xNC4ZEd/swrMk9Z9Zg7G9UHeoDy7N3II/201FxIJdaDCmJ+oN66LLSxGlrr19MG3ORHy/7Cf07zQU0ddj8OOu72AjtVZb39OrCZasnYd9Ow6in88QHD10Aqs2L0ad+m7KOis3L0b1Ws4YP3Qq+vl8ioT4RPy0dxVMzUwAAIkPk9CucTeVY/Widch+mo1TIWFlct1i4ti7FerP+RQxy/6HM5388eT6fXjt8oeRVP33mJ6pEZ7dT0b0gh14XsT3WONvR8L23SaIHLcGp9tPRVpoJJrvnQVjR/X/v6is5Do8xIIDnGJq/0UPnNl1FOf2HkdSzEPsmbkBec/y0PKj9mrrx0XexW9BP+PywTAU5BUU2a5MJsOTlMfKIzvjiY6uoPJxH9ENd3YcQ+zuE8i6/RAXpm9EwbNcuA1qp7Z+esRdXJm3E3EHzkJWxGcm9aqH+MOXkBByBdnxqXjwx3kkHr8KW083tfWpaENHDcL/th/A/l2/486tWMyZuhDPnz3Hh4N6qa3/yYgBOHXsLDZ9vx13b9/DqkU/IupqND7+vD8AoJZbDXh6NcHc6Ytw7coN3LsTh7nTFsHY1Bjd+3QGAMjlcqSmpKscHbu3Q/BvIcjJeVZm1y4WLqN64MH2o3i46ziybz3E9akbIHuWB+dB7dXWz7pyF9Fzf0bi/jAocl/9HtMzMYRDjxa4NW8HMs7eRM69JMQs/R9yYhNR87NOOr4aEps3GuBERUUhODgYv/32m8ohNvqG+qjR2BW3Tl9VlikUCtw6fRUub9d7o7btXBwx99z3mH3iO3y6Yhysq9m+aXcJgJ6hPmw8XJF48tqLQoUCSSevQdqsrtbtpl68BYc2jWDu5ggAsGpYE3Yt3JFwNOJNu1ypGBoaoKFHfYSdPK8sUygUOHviApp6NVF7jmezJjh74oJK2eljZ+H5T30jYyMAQN7zPJU283Lz8XaLpmrbbOhRHw2auGPfz+L7uaVrEkN9WHi4Iu3ki5+LUCiQduIqrLy0+7ko0deHnoE+ZLn5KuXy53mwblH/TborOgqJ7g6x0GoNzt27d9GnTx9cvXoVEokECkXhjnyJpPC/jEwme+35ubm5yM3NVSkrUMhgINHXpjs6V8XaAvoG+niS+lil/EnKY9jXdta63ftXYrDjyx+QfPcRLOyt0HViP0zY8zUWdpmK3Oznb9rtSs3Yxhx6Bvp4nqL6mT1PzYJ5nWpatxu1+iAMzU3R48QSKGRySPT1ELlwL+7/ekbzyaRkZWMFAwMDpKWkq5SnpaTDtW4ttedI7W3V1re1L/yjIPb2PSQ8eIRJM8dgztSFeJbzDENGDoKTswPsHKRq2+z7cS/ciY7FlYtX1b5ORTOysYCegT7yXvoey015jCp1tfu5KMt+jowLt1Bn8oeIuPUQuSmZcOrTGlZe9ZATm1ga3aZKRKsEZ+LEiXB1dUVycjLMzMxw/fp1nDhxAl5eXggNDdV4flBQECwtLVWOi49vaNMVQbsRegVX/jyHhJtxuHkiEj/6LoSpRRW81aNVeXeNilDzfW/U+rA1zoxdg+Aus3B24o+oP6o7XPu3Le+uVXoFBTJM/HwGXGrXRNitv3Hx3nG0aN0MJ/4+A7n81ZUFxibG6P5hF/yyg+lNRRI5dg0gAd6L/AGdH2xHreFd8ejX01DIeWu7/+IaHM20SnDCwsJw9OhRSKVS6OnpQU9PD23atEFQUBAmTJiAy5cvv/Z8f39/+Pn5qZY1GaZNV8pEdkYWZAUymEstVcrN7SzxJCWz1N7nWVYOUmIfQeriUGptVla56U8gL5DBxE71MzORWryS6pSE5+yPcWP1QcQdOAsAeHzzAapUl6Lh+PcRu/fkG/W5MslMz0RBQQFs7WxUym3tbJCanK72nNTkNLX105LTlF9HRd5E346foqp5FRgaGSIjLRM7D/2E61duvtJe554dYGpqgt/2/lkKV1T55KVnQV4gg9FL32PGdpbITc7Uut1n95Nwvs9c6JsZw6CqKXKTM9F03UTk3E96wx5TZaNVgiOTyWBubg4AkEqlSEhIAADUqlUL0dHRGs83NjaGhYWFylFRp6cAQJYvw4Nrsaj3TmNlmUQiQb13GuNe+K1Sex8jM2PY1nJA1hv8cKBC8nwZ0iNj4dim0YtCiQQObRoj9dJtrds1MDGC4qU0QCGTAxIRTVyXgfz8AkRF3kTLts2VZRKJBN5tmyOiiOmiK5euomVbL5WyVu1aqJ1eevokGxlpmajpWgONmjbA0eATr9T58ONeOHb4JDLSMt/sYiopRb4MWZGxsG374uciJBLYtm2MzItv/nNRlpOL3ORMGFhWgbS9B5IPX3rjNsWECY5mWiU4jRs3RkREBFxdXeHt7Y3FixfDyMgI69atg5ubOHeThG74A4OXjUbc1buIuxKDdsO6w8jMGOf2HgcADF42Bo+T0vH74l0AChcmO9atDgAwMNSHpYMNnBvWQm72c6T+85dI768+wbWQS8h4mAoLe2t0n9wPCpkcl347XT4XKTLR6w6h5YqRSI+IRdrlO3Af3hUGZsaI3VX4mbX8bhSeJWYgImg3gMKFyRb1qv/zbwOYOlnDqlEtFGQ/x9N7hZ/Zw78uo9GED5DzMA2Po+Nh3dgF7iO74e4/bVLxbVm7E9+sDMD1Kzdw9XIUPh0xEKZmJvh11+8AgG9WBSI5MQUrFnwPANi+bjc271+LoaM+xom/T6PbB53QuGkDfP1lkLLNzr06ICMtE48eJqJugzrwnzcZRw+dwJnj51Teu6ZLdXi1egujP55cdhcsQvfW/oEmK0fj8ZW7eHw5Bi4jukPfzBgP//l+aLJqDHIT03FrQeHPRYmhPqr+8z0mMdKHiaMNzBvVgiz7OXL++R6TtvcAJBJk30mAmYsj3AMHIzsmAQ93hpbLNZJwaTXAmTVrFrKzswEAc+fORc+ePdG2bVvY2tpi9+7dpdrBiuLy72GoamOB7pP7w8LOCvE37mPt0IXKhcfWzlLlYmsAsHSwwbQ/Fym/7jiyFzqO7IXbZ6OweuBcAICVkw2GrhyPKlbmeJqehbsXo7G8z2xkp3OreGmI++0sjG3N0WRqP5jYWSLj+n2EDl6E56lZAAAzZ1uVeX1TB2t0++sb5dcNRvdEg9E9kXQmCkf7LQAAXJq1BR7T+sEryBfGthZ4lpSBmG1Hcf3bfWV7cSIQfOBv2NhaYdy0EZDa2+Lm9VsYOWiSciGxk7ODSlp25eJVTBs9GxNmjMKkr0bjfuwDjP9sGmJuvrgBnJ2DFNPmTILUzgYpSan4be8hrF3+0yvv3efjXkhKSMbp0HOvvEbFl3ggDEa2Fqg7rT+M7a2Qdf0+Lg5aqFx4bOosBf7zPWbiaIPWR1/8XHQd2wuuY3sh/XQUzn9Y+HPRwMIM9WYOgomTDfIynyLp9/O4HbQLioLXb16pbLgiSTOJ4r+/ld9Aeno6rK2tlTupSmqiy8DS6AaVkZZ5FXdKkdSbL4sp7y5QCSyDa3l3gUqoa9KuMnuv72p+orO2J8Zt11nbZanUHtVgY2OjuRIRERFRGeCzqIiIiARGTIuBdYWPaiAiIiLRYYJDREQkMExwNGOCQ0RERKLDBIeIiEhguE1cMyY4REREJDpMcIiIiARGzqfDaMQBDhERkcBwkbFmnKIiIiIi0WGCQ0REJDBcZKwZExwiIiISHSY4REREAiNnhqMRExwiIiISHSY4REREAsNdVJoxwSEiIiLRYYJDREQkMFyBoxkHOERERALDKSrNOEVFREREosMEh4iISGD4LCrNmOAQERGR6DDBISIiEhje6E8zJjhEREQkOkxwiIiIBIb5jWZMcIiIiEh0mOAQEREJDO+DoxkTHCIiInoja9asgYuLC0xMTODt7Y3z588XWff69evo27cvXFxcIJFIsGLFilfqfP3115BIJCpH/fr1S9QnDnCIiIgERg6Fzo6S2r17N/z8/BAYGIjw8HA0bdoUXbp0QXJystr6OTk5cHNzw8KFC+Ho6Fhku40aNcKjR4+Ux6lTp0rULw5wiIiIBEahw6Okli9fjuHDh8PX1xcNGzbE2rVrYWZmho0bN6qt37x5cyxZsgQDBw6EsbFxke0aGBjA0dFReUil0hL1iwMcIiIiUsrNzUVWVpbKkZubq7ZuXl4eLl26BB8fH2WZnp4efHx8EBYW9kb9uH37NqpVqwY3NzcMHjwYcXFxJTqfAxwiIiKBkevwCAoKgqWlpcoRFBSkth+pqamQyWRwcHBQKXdwcEBiYqLW1+ft7Y3NmzcjODgYP/zwA2JjY9G2bVs8efKk2G1wFxUREREp+fv7w8/PT6XsdVNJutCtWzflvz08PODt7Y1atWphz549GDZsWLHa4ACHiIhIYHT5qAZjY+NiD2ikUin09fWRlJSkUp6UlPTaBcQlZWVlhXr16iEmJqbY53CKioiIiLRiZGSEZs2aISQkRFkml8sREhKCVq1aldr7PH36FHfu3IGTk1Oxz2GCQ0REJDAV6VENfn5+GDp0KLy8vNCiRQusWLEC2dnZ8PX1BQAMGTIEzs7OynU8eXl5iIqKUv774cOHuHLlCqpWrYo6deoAAL788kv06tULtWrVQkJCAgIDA6Gvr49BgwYVu18c4BAREZHWBgwYgJSUFAQEBCAxMRGenp4IDg5WLjyOi4uDnt6LCaOEhAS89dZbyq+XLl2KpUuXol27dggNDQUAxMfHY9CgQUhLS4OdnR3atGmDs2fPws7Ortj94gCHiIhIYCraoxrGjRuHcePGqX3t30HLv1xcXKBQvD6D2rVr1xv3iQMcIiIigVFUqEmqiomLjImIiEh0mOAQEREJTEWboqqImOAQERGR6DDBISIiEhhd3uhPLJjgEBERkegwwSEiIhIY5jeaMcEhIiIi0WGCQ0REJDBcg6MZBzhEREQCw23imnGKioiIiESHCQ4REZHA8FENmjHBISIiItFhgkNERCQwXIOjGRMcIiIiEp0Kk+Acyo4p7y5QCVwztinvLlAJNTZ0Ku8uUAlY5eeVdxeoAuMaHM2Y4BAREZHoVJgEh4iIiIqHa3A04wCHiIhIYOQKTlFpwikqIiIiEh0mOERERALD/EYzJjhEREQkOkxwiIiIBIZPE9eMCQ4RERGJDhMcIiIigeGN/jRjgkNERESiwwSHiIhIYHijP804wCEiIhIYLjLWjFNUREREJDpMcIiIiASGi4w1Y4JDREREosMEh4iISGC4yFgzJjhEREQkOkxwiIiIBEah4BocTZjgEBERkegwwSEiIhIY3gdHMw5wiIiIBIaLjDXjFBURERGJDhMcIiIigeGN/jRjgkNERESiwwSHiIhIYLjIWDMmOERERCQ6THCIiIgEhjf604wJDhEREYkOExwiIiKB4X1wNOMAh4iISGC4TVwzTlERERGR6DDBISIiEhhuE9eMCQ4RERGJDhMcIiIigeE2cc2Y4BAREZHoMMEhIiISGK7B0YwJDhEREYkOExwiIiKB4X1wNOMAh4iISGDkXGSsEaeoiIiISHSY4BAREQkM8xvNmOAQERGR6DDBISIiEhhuE9eMCQ4RERGJDhMcIiIigWGCoxkTHCIiIhIdJjhEREQCw4dtaqbVAMfa2hoSieSVcolEAhMTE9SpUwefffYZfH1937iDRERERCWl1QAnICAACxYsQLdu3dCiRQsAwPnz5xEcHIyxY8ciNjYWo0ePRkFBAYYPH16qHSYiIqrsuAZHM60GOKdOncL8+fMxatQolfIff/wRR44cwS+//AIPDw+sXLmSAxwiIqJSxmdRaabVIuPDhw/Dx8fnlfKOHTvi8OHDAIDu3bvj7t27b9Y7IiIiIi1oNcCxsbHBwYMHXyk/ePAgbGxsAADZ2dkwNzd/s96Vs8Gf98fRS7/h6oPT2Bu8GR5vNXpt/a7vd0Twmf/h6oPTOHh8F9r5tFZ53dbOBgtXBeLk1UOIuH8KG3avRC23Gip15i79Cn+f34/IuFM4e+MvfL91Gdzq1Cr1a6sseg/thZ/DtuJQzO9YfXAl3D3di6xbq14tBK6bjZ/DtiIk/gg+HNbnlTpNvJtg/qa52H1xJ0Lij6B1l3d02f1Kp/OQblh1ah22Re/B/P2LUbtp3SLrVq9bA35rp2PVqXXYfX8/un/e65U6nT7pisXBK7Dp2g5surYD835dCM/2b+vyEio9h8+64q1za9Hi7i40/n0hqnjWKbKu/cc+aPjrfHhFbYVX1FY02B342vr0gkKh0NkhFloNcGbPno2pU6fi/fffx/z58zF//nz07t0b06ZNQ2BgIADgr7/+Qrt27Uq1s2Wp+wed4D93MlYvXY8POn6Cm9dv4ac9q2AjtVZb/63mHlj+4wLs/fkAPugwGH8fCsWaLUtRt35tZZ3vtyxFjVrOGPPpFHzQYTASHiRi8/++h6mZibLO9YgbmDFxDrq17o/PB4yDRCLBxr1roKfHHf0l1b5XO4wKGImt327HqG5jcCfqLhZt/wZWtlZq65uYGuNRXCI2BG1EWlKa2jqmZia4E3UXK2et1mHPK6dWPVtjyKzP8ct3uzCjpx/u37iHr7YFwsLWUm19Y1NjJMUlYueirchITldbJ+1RGnYs2gb/nlPwVa8vce3MVUxd74/qdWuorU9vxvb91qgV6Iv45XtwtcuXyI66hwY7AmBQxGdo8U5jpO0/haj+Abj2vj9yE9LQYGcgDB1tyrjnJEYShZbDtdOnT2P16tWIjo4GALi7u2P8+PF45x3t/qKtZ+el1Xm6sjd4M65eicLcGYsBFO4QOxHxB7Zt2I11K7e8Un/F+m9gamaKkYMnK8v2HNqEG9duIXBqEFzcauLIuX3o3uYjxETfVbZ55vphLP9mDfZuP6C2H+4N6+Dg8V3o2Lw3Htx7qIMr1U4N44r/A2j1wZWIjojGqllrABT+99514Wf8uukAdq3Z/dpzfw7bil82/Ip9P/1aZJ2Q+CMIGPY1Th8+U6r91hWpvll5d+G15u9fjDuRt7EpYD2Aws/r+7MbELz5Dxz4Yd9rz111ah0ObTyIPze+miy/7KeIbdj+zRYc2/13qfRbVybnC++Pmsa/L8TTiBjcm7mhsEAiwdsX1yFx059IWF3095KSnh68bmzFvZkbkPq/UJ32VRdaJrz+/6el6W2nNjprO/zRKZ21XZa0/g5q3bo1du7cifDwcISHh2Pnzp1aD24qGkNDAzRqWh9njp9TlikUCpw5cR6eXh5qz/H08sCZE+dVyk4dC8NbXk0AAEbGhgCA3NxclTbz8vLQzNtTbZumZib4cND7eHAvHokPk97kkiodA0MD1GtSF+EnLyvLFAoFwk9eRsO3G5Rjz0gdfUMDuDWpjaunIpVlCoUCV09FoO7bRU8rloRETw/v9GoDY1MT3Aq/WSpt0gsSQwNU8aiNxydffIZQKPD4ZCSqNiveZ6hnagQ9A30UZD7RUS9JV9asWQMXFxeYmJjA29sb58+fL7Lu9evX0bdvX7i4uEAikWDFihVv3KY6Wg9wZDIZfvnlF+UU1a+//gqZTKZtcxWKtY0VDAwMkJqiGnunJqfDzt5W7TlSe1ukvhSTp6akQ/pP/bu37+Hhg0eYMmscLCzNYWhogOHjh8LJ2RF2DlKV8z727YfL904g4v4ptOv4Dj7rPxb5+QWleIXiZ2ljAX0DfWSkZKiUZ6RmwMa+4qdPlY2FtTn0DfTxODVTpfxx6mNY2amfFi6uGu61sCVqJ36+vRdfLBiNpSMX4uHt+Ddqk15lYGMOiYE+8lMyVcrzUzNhZGdVrDZqzhyCvKQM1UESqVWR1uDs3r0bfn5+CAwMRHh4OJo2bYouXbogOTlZbf2cnBy4ublh4cKFcHR0LJU21dFqgBMTE4MGDRpgyJAh2LdvH/bt24dPPvkEjRo1wp07dzSen5ubi6ysLJVDrpBr0xXBKCiQYdxnU+FauyYuxhxDRNwpeLdphuN/n4ZCrnrtv/3vED7oMBgfvz8csXfi8N2GhTAyNiqnnhMJW8Ldh5jWbTJm9p6Gv7YfwthlE+Bct3p5d4teUm1cH0h7t8atYYugyM0v7+5Uaup+R/939uFly5cvx/Dhw+Hr64uGDRti7dq1MDMzw8aNG9XWb968OZYsWYKBAwfC2Ni4VNpUR6sBzoQJE1C7dm08ePBAOUUVFxcHV1dXTJgwQeP5QUFBsLS0VDkychK16YpOZKRnoqCgAFI71b/0pfY2SElWv/g0NTkN0peSAamdDVL/U/965E30fm8w3nZrh9aNu+KLARNgZW2JB/dV19Y8fZKN+3cf4GLYZUz4fBrc6rigU/f3SunqKofH6VmQFchg/dJf/9ZSa6QXsSCVyk9WxhPICmSwlFqplFtKLZH5UgpXUrL8AiTdT0TstTvYuXg77t+4h+6+r+64ojdTkP4EigIZDF9KawylVsh7KdV5mdOo3qg29kPcGDQXOTfu666TIiKHQmeHut/RQUFBavuRl5eHS5cuqdw6Rk9PDz4+PggLC9Pq2kqrTa0GOMePH8fixYuVW8IBwNbWFgsXLsTx48c1nu/v74/Hjx+rHNZm6mOq8pCfX4DrETfR6t0WyjKJRIJWbZvjykX10emVi5Fo1ba5Stk77bxx+eLVV+o+fZKNjLRM1HKrgcaeDfD3odf8N5NIIJFIlGt4qHgK8gtw6+ptvNXGU1kmkUjwVhtPRIXfKL+OkVqy/ALcvXoHTVq/WOMmkUjQuLUHbodHl+p7SfQkMDDi91NpU+QXIDvyDizb/GedokQCizYeeHqp6M/QacwHcJ7UDzcHz0N2pOYZACqk0OH/1P2O9vf3V9uP1NRUyGQyODg4qJQ7ODggMVG74KK02tTqTsbGxsZ48uTVRWBPnz6FkZHmqRRjY+NXYik9ScXaMbBp7c9YtOprXLsShcjw6xg68mOYmpnil52FuzQWr56DpMRkLJtfuENny7pd2H5gHT4fPRihf51Cjz5d0NizIWZP+UbZZtf3OyI9NROPHiaiXoM6mLlgCv4+dBynQwsXM9eo5YzuH3TCqWNnkZ6WAcdqDhgx4TM8f/4cx/8+Xfb/EQTuf+t+wfRvp+JWxG3cvHITfb/4ECamJji8u/BmlNNXTEVqYhp+WlgYeRoYGqBW3Zr//NsQUicpajd0w7Oc50i4lwAAMDEzgbNLNeV7ONZwRO2GbniS+QTJCSllfIXi8seGAxizbCLuRMbgTsRtdP+8F4zNTBC6NwQAMHb5RKQnpmHn4u0AChcm/7vd28DIANaONqjV0BXPs58h6X7hD8FB0z7BldBwpCakwqSKKdr0bouGLRvjm0/nlM9FityjdQdRe8V4PI2IwdPLt+E0vBf0zYyRsusoAKD2dxOQl5iGB0E/AwCqje2D6l8ORMzYb5H7IFmZ/siyn0Oe87y8LqPSU/c7Woi0GuD07NkTI0aMwE8//aR8FtW5c+cwatQovP/++6XawfLy5/6/YGNrjQnTR8HO3hY3rt3CsAHjkfbPwmOn6o4q64YuX4jElFEzMcl/DPxmjsW9uw8wduiXuH3zxV8kdg5S+M+dDFs7W6QkpWL/nj/w/bINytdzn+fCq+VbGDpiECysLJCWkoYLYZcxsPswpKe+WUxfGYUePA5LW0t89uUQWNtZ407UXcz4dCYy/lnIau9sD4X8xYI6WwdbrDuyVvn1gFH9MWBUf1wJi8CU/lMBAO5N62H53qXKOmO+LnxcyeE9R7DY70U5lVzY76dhYWuJj/wGwcrOGveiYhE0ZA4epz4GANhWs4P8P5+XjYMNFh/6Vvn1+yP74P2RfXA97BrmDpwFALCQWmHM8kmwtrdGzpNsxN28j28+nYOrpyLK9uIqibTfTsPA1gI1pg6CoZ0Vcq7H4ubgecj/5zM0dpYC/1lz6DCkC/SMDVFvwzSVduKX7Ub8stffyqGyk1eQG/JJpVLo6+sjKUl1p29SUlKRC4jLqk2t7oOTmZmJoUOH4uDBgzA0LIx68/Pz0bt3b2zatAlWVlYlbbLC3QeHXk8I98EhVRX9PjikSoj3wansyvI+OI0dWuqs7WtJZ0tU39vbGy1atMCqVasAAHK5HDVr1sS4ceMwY8aM157r4uKCSZMmYdKkSaXW5r+0SnCsrKxw4MABxMTE4MaNwvUMDRo0QJ06vMU2ERGRrlWkh236+flh6NCh8PLyQosWLbBixQpkZ2fD19cXADBkyBA4OzsrFyrn5eUhKipK+e+HDx/iypUrqFq1qnIcoanN4ij2AMfPz++1rx87dkz57+XLlxe7A0RERCRcAwYMQEpKCgICApCYmAhPT08EBwcrFwnHxcWpPG4oISEBb731lvLrpUuXYunSpWjXrh1CQ0OL1WZxFHuK6r33VLcph4eHo6CgAO7uhXeovHXrFvT19dGsWTMcPXq02B34F6eohIVTVMLDKSph4RSV8JTlFFUD+xaaK2npRnLJ7hhcURU7wXk5oTE3N8eWLVtgbV14n5GMjAz4+vqibdu2pd9LIiIiohLQapGxs7Mzjhw5gkaNGqmUX7t2DZ07d0ZCQkKJO8IER1iY4AgPExxhYYIjPGWZ4NS3b665kpZuJl/QWdtlSatFxllZWUhJefWeHykpKWrvj0NERESlp6JsE6/ItPoToU+fPvD19cW+ffsQHx+P+Ph4/PLLLxg2bBg+/PDD0u4jERERUYloleCsXbsWX375JT7++GPk5xc+FM3AwADDhg3DkiVLSrWDREREpKoibROvqLQa4JiZmeH777/HkiVLlE8Pr127NqpUqVKqnSMiIiLShlYDnH9VqVIFHh4emisSERFRqeEaHM24TJ+IiIhE540SHCIiIip7XIOjGRMcIiIiEh0mOERERAKjUMjLuwsVHgc4REREAiPnFJVGnKIiIiIi0WGCQ0REJDBaPEay0mGCQ0RERKLDBIeIiEhguAZHMyY4REREJDpMcIiIiASGa3A0Y4JDREREosMEh4iISGD4sE3NOMAhIiISGD6LSjNOUREREZHoMMEhIiISGC4y1owJDhEREYkOExwiIiKB4Y3+NGOCQ0RERKLDBIeIiEhguAZHMyY4REREJDpMcIiIiASGN/rTjAMcIiIigeEUlWacoiIiIiLRYYJDREQkMNwmrhkTHCIiIhIdJjhEREQCwzU4mjHBISIiItFhgkNERCQw3CauGRMcIiIiEh0mOERERAKj4C4qjTjAISIiEhhOUWnGKSoiIiISHSY4REREAsNt4poxwSEiIiLRYYJDREQkMFxkrBkTHCIiIhIdJjhEREQCwzU4mjHBISIiItFhgkNERCQwTHA04wCHiIhIYDi80YxTVERERCQ6EgVzLp3Jzc1FUFAQ/P39YWxsXN7dIQ34eQkPPzNh4edFZYkDHB3KysqCpaUlHj9+DAsLi/LuDmnAz0t4+JkJCz8vKkucoiIiIiLR4QCHiIiIRIcDHCIiIhIdDnB0yNjYGIGBgVxMJxD8vISHn5mw8POissRFxkRERCQ6THCIiIhIdDjAISIiItHhAIeIiIhEhwMcIiIiEh0OcHSoffv2mDRpUnl3g4iozPHnH5U3Pk1ch/bt2wdDQ8Py7gYREVGlwwGODtnY2JR3F4iIiColTlHpECPaiiU4OBht2rSBlZUVbG1t0bNnT9y5cwcAcO/ePUgkEuzZswdt27aFqakpmjdvjlu3buHChQvw8vJC1apV0a1bN6SkpJTzlRDw+s+TylZ2djaGDBmCqlWrwsnJCcuWLVN5PSMjA0OGDIG1tTXMzMzQrVs33L59W6XO+vXrUaNGDZiZmaFPnz5Yvnw5rKysyvAqSGw4wKFKIzs7G35+frh48SJCQkKgp6eHPn36QC6XK+sEBgZi1qxZCA8Ph4GBAT7++GNMmzYN3333HU6ePImYmBgEBASU41XQv4rzeVLZmDp1Ko4fP44DBw7gyJEjCA0NRXh4uPL1zz77DBcvXsRvv/2GsLAwKBQKdO/eHfn5+QCA06dPY9SoUZg4cSKuXLmCTp06YcGCBeV1OSQWCtKZdu3aKSZOnFje3aAipKSkKAAorl69qoiNjVUAUGzYsEH5+s6dOxUAFCEhIcqyoKAghbu7e3l0lzT47+dJZefJkycKIyMjxZ49e5RlaWlpClNTU8XEiRMVt27dUgBQnD59Wvl6amqqwtTUVHnOgAEDFD169FBpd/DgwQpLS8syuQYSJyY4VGncvn0bgwYNgpubGywsLODi4gIAiIuLU9bx8PBQ/tvBwQEA0KRJE5Wy5OTksukwvVZxPk/SvTt37iAvLw/e3t7KMhsbG7i7uwMAbty4AQMDA5XXbW1t4e7ujhs3bgAAoqOj0aJFC5V2X/6aqKS4yJgqjV69eqFWrVpYv349qlWrBrlcjsaNGyMvL09Z57+73iQSidoyToFUDMX5PImo8mKCQ5VCWloaoqOjMWvWLHTs2BENGjRARkZGeXeLtMTPs+KoXbs2DA0Nce7cOWVZRkYGbt26BQBo0KABCgoKVF7/9/Nr2LAhAMDd3R0XLlxQafflr4lKigkOVQrW1tawtbXFunXr4OTkhLi4OMyYMaO8u0Va4udZcVStWhXDhg3D1KlTYWtrC3t7e8ycORN6eoV/P9etWxe9e/fG8OHD8eOPP8Lc3BwzZsyAs7MzevfuDQAYP3483n33XSxfvhy9evXC0aNHcejQIWWKSqQNJjhUKejp6WHXrl24dOkSGjdujMmTJ2PJkiXl3S3SEj/PimXJkiVo27YtevXqBR8fH7Rp0wbNmjVTvr5p0yY0a9YMPXv2RKtWraBQKPDnn38qp39bt26NtWvXYvny5WjatCmCg4MxefJkmJiYlNclkQhIFAqForw7QURE9F/Dhw/HzZs3cfLkyfLuCgkUp6iIiKjcLV26FJ06dUKVKlVw6NAhbNmyBd9//315d4sEjAkOERGVu48++gihoaF48uQJ3NzcMH78eIwaNaq8u0UCxgEOERERiQ4XGRMREZHocIBDREREosMBDhEREYkOBzhEREQkOhzgEBERkehwgENERESiwwEOERERiQ4HOERERCQ6/we++f6e7jcTLgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the highest relationship between words is given by the words \"I\" and \"am\" which makes sense since they are strongly related. The differences are small, but also bear in mind that this is very early stage in the attention mechanism, and there are 12 encoding blocks stacked togetehr as we saw above."
      ],
      "metadata": {
        "id": "pgytZAo6IcLU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u01m2IzIH7Wf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}